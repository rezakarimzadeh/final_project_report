
\فصل{روش‌های پیشنهادی و نتایج}

\قسمت{مقدمه}
در این فصل به معرفی روش‌های ارایه شده در این مطالعه، برای قطعه‌بندی ساختارهای در ریسک و تومور با استفاده از شبکه‌های عصبی عمیق پرداخته خواهد شد. مطالعه شامل دو بخش است که در بخش اول به قطعه‌بندی ساختارهای در ریسک، چالش‌های موجود، راهکارهای پیشنهادی و ارزیابی روش‌های پیشنهادی پرداخه می‌شود و در بخش دوم مطالعه همین روند برای قطعه‌بندی تومور طی خواهد شد.

\قسمت{روش‌های پیشنهادی قطعه‌بندی ساختارهای در ریسک}

همانطور که در فصل اول اشاره گردید، ضرورت قطعه‌بندی ساختارهای در ریسک، در تهیه‌ی نقشه‌ی درمان برای رادیوتراپی است. این نقشه‌ی درمان کمک می‌کند تا به بافت‌های سالم کنار تومور (ساختارهای در ریسک) کمترین آسیب و تومور بیشترین دوز پرتو را دریافت کند. بنابراین لازم است این ساختارها با دقت  و سرعت بالا قطعه‌بندی شوند و در روند درمان قرار گیرند. این قطعه‌بندی به صورت دستی زمان‌بر، پرهزینه (هزینه‌ی استخدام شخص متخصص)، خسته‌کننده و متغیر بر اساس دیدگاه هر متخصص است.

بنابراین برای تسریع این روند و حل مشکلات ذکر شده، روش‌های خودکار و نیمه‌-خودکار فراوانی ارایه گردید که در فصل قبل به بررسی هر یک پرداخته شد. ملاحظه گردید که الگوریتم‌های کلاسیک ارایه شده، در اکثر موارد توانایی قطعه‌بندی چند ساختار به صورت همزمان را ندارند و علاوه بر آن در اکثر موارد نیاز به یک مقدار (کانتور) اولیه دارند که این کاستی‌ها در کنار دقت پایین باعث محبوبیت پایین این روش‌ها شده است.

در دست دیگر، روش‌های قطعه‌بندی مبتنی بر یادگیری عمیق بررسی شد که با آموزش این مدل‌ها بر اساس یک مجموعه داده، علاوه بر سرعت و دقت بالا نسبت به روش‌های قطعه‌بندی کلاسیک و دستی، امکان قطعه‌بندی خودکار چند ساختار نیز وجود دارد. بنابراین شاهد استفاده‌ی روز افزون این روش‌ها و افزایش اعتبار و محبوبیت آن‌ها برای استفاده در نرم‌افزارهای قطعه‌بندی ساختارهای در ریسک در مراکز پرتو درمانی هستیم.

در این بخش به توضیح روش‌های پیشنهادی برای قطعه‌بندی ساختارهای در ریسک  بر اساس مدل‌های یادگیری عمیق پرداخته می‌شود. ابتدا مجموعه دادگان و پیش‌پردازش‌های انجام شده، توضیح داده خواهد شد و در ادامه، روش‌های پیشنهادی معرفی می‌شوند و نتایج این روش‌ها بررسی خواهد شد.

\زیرقسمت{معرفی مجموعه دادگان و پیش‌‌پردازش}
\label{segthor}
در این قسمت، دو مجموعه داده‌ برای قطعه‌بندی ساختارهای در ریسک معرفی می‌شود که در ادامه به بررسی هر یک و پیش‌پردازش‌های انجام شده، پرداخته خواهد شد.
\زیرزیرقسمت{مجموعه داده‌ی SegTHOR}

مجموعه داده‌ی SegTHOR شامل قطعه‌بندی چهار ساختار در ریسک قفسه‌ی سینه در تصاویر سی‌تی اسکن است\LTRfootnote{َSegmentation of THoracic Organs at Risk}. این مجموعه تصاویر، از 40 بیمار دارای سرطان ریه و مری گرفته شده است و شامل قطعه‌بندی دستی چهار ساختار در ریسک قلب\LTRfootnote{Heart} ،آئورت\LTRfootnote{Aorta}، نای\LTRfootnote{Trachea} و مری\LTRfootnote{Esophagus} است که به عنوان استاندارد مطلوب\LTRfootnote{Ground Truth} برای آموزش شبکه‌های عصبی تهیه شده است.

این مجموعه داده برای قطعه‌بندی ساختارهای در ریسک به دلیل تغییر شکل ساختار‌های در ریسک (مانند مری) در هر شخص و نیز اختلاف شدت پایین در میان بافت‌های نرم در تصاویر سی‌تی اسکن، بسیار پر چالش است \مرجع{lambert2020segthor}. شکل ~\رجوع{شکل:سگتوراوریجینال} یک نمونه از تصاویر اولیه‌ی موجود در این مجموعه داده را نشان می‌دهد که به علت کمبود اختلاف شده‌ها به سختی بدن بیمار قابل تشخیص است. 

\شروع{شکل}[H]
\centerimg{04segthorrawimg.png}{8cm}
\شرح{نمونه‌ی یک تصویر سی‌تی اسکن از مجموعه داده‌ی SegTHOR در سه نمای اکسیال، کرونال و سجیتال}
\برچسب{شکل:سگتوراوریجینال}
\پایان{شکل}

این تصاویر دارای ابعاد اولیه‌ی $512*512*(150\sim 284)$ و با فاصله‌ی واکسلی\LTRfootnote{Voxel Spacing}،
 $(0.97\sim1.36)*(0.97\sim1.36)*(2\sim2.5)$
 در راستاهای x، y و z است که حاکی از متفاوت بودن دستگاه‌های تصویر برداری در این مجموعه دادگان است. بنابراین لازم است برای تمایز هرچه بهتر بافت‌ها و یکسان‌سازی تصاویر دستگاه‌های مختلف در جهت آموزش شبکه‌ی عصبی عمیق، یک پیش‌پردازش مناسب لازم است.
 
اولین گام در پیش‌پردازش یکسان‌سازی فاصله‌ی واکسلی به $0.97*0.97*2$ است که با استفاده از درون‌یابی برای تصویر اصلی و تصویر قطعه‌بندی متناظر این کار انجام گردید و تا حدی تفاوت تصویربرداری در بین دستگاه‌های مختلف یکسان‌سازی شد. در گام دوم، ناحیه‌ی شامل ساختارهایی که باید قطعه‌بندی شوند از تصویر اصلی استخراج گردید و سایر نواحی مانند قسمت‌های شکمی و سر و گردن حذف شد. در گام سوم مقدار \LTRfootnote{Hounsfield Unit}HU در تصاویر اصلی که در بازه‌ی $(-1000\sim 13000)$ بود مقادیری که در خارج از بازه‌ی $(-800\sim 400)$ بود، به سقف و کف این بازه تبدیل داده شد و مقادیر این بازه در تصاویر اصلی بین صفر و یک نرمالیزه گردید. در نهایت فضاهای خالی اطراف تصویر به مرکزیت آن بریده شد و ابعاد $512*512$ به $384*288$ کاهش یافت. شکل زیر تصویر نشان داده شده در شکل ~\رجوع{شکل:سگتوراوریجینال} را بعد از پیش‌پردازش نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04segthorprepimg.png}{8cm}
\شرح{نمونه‌ی یک تصویر سی‌تی اسکن از مجموعه داده‌ی SegTHOR بعد از پیش‌پردازش، در سه نمای اکسیال، کرونال و سجیتال}
\برچسب{شکل:سگتورپیریپراسسد}
\پایان{شکل}

برای حفظ تناظر یک به یک بین تصاویر اصلی و قطعه‌بندی استاندارد، تغییرات و پردازش‌های اعمال شده در بالا، مطابق با ماهیت برچسب‌ها، اعمال گردید تا دوباره ماسک‌های قطعه‌بندی شده در روند آموزش قابل استفاده گردند. شکل ~\رجوع{شکل:سگتورویتلیبل} نمونه‌ای از تصویر اصلی همراه با برچسب‌های چهار ساختار در ریسک را نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04segthorprepimg_withlabel.png}{8cm}
\شرح{نمونه‌ی یک تصویر سی‌تی اسکن از مجموعه داده‌ی SegTHOR بعد از پیش‌پردازش همراه با برچسب ساختار در ریسک، در چهار نمای اکسیال، کرونال، سجیتال و سه‌بعدی}
\برچسب{شکل:سگتورویتلیبل}
\پایان{شکل}

برای آموزش شبکه (شبکه‌های به صورت دوبعدی آموزش داده می‌شوند) و افزایش قدرت تعمیم‌پذیری آن از روش افزایش مجموعه دادگان استفاده شد که در اینجا به معرفی جزییات آن پرداخته می‌شود. هر تصویر به صورت دو بعدی و از نمای اکسیال از تصویر سه‌بعدی پردازش شده با برچسب ساختارهای در ریسک متناظر استخراج می‌گردد و با اعمال تبدیلات دوران تصادفی بین زاویه‌ی $(-5,5)$ درجه، بزرگ‌نمایی و کوچک‌نمایی با ضرایب $1.1$ و $0.9$ و قرینه‌ی افقی تعداد دادگان آموزش افزایش می‌یابد. در شکل ~\رجوع{شکل:سگتوراگمنت} (الف) تصویر دوبعدی اصلی با برچسب‌ متناظر نشان داده شده است و در (ب) قرینه‌ی افقی و بزرگ‌نمایی آن با ضریب $1.1$ به تصویر کشیده شده‌است.

\شروع{شکل}[H]
\centerimg{04segthoraugmentation.png}{8cm}
\شرح{نمونه‌ی یک تصویر در نمای اکسیال با برچسب ساختار در ریسک متناظر (الف) تصویر اصلی (ب) تصویر تبدیل یافته برای افزایش دادگان}
\برچسب{شکل:سگتوراگمنت}
\پایان{شکل}

\زیرزیرقسمت{مجموعه داده‌ی قطعه‌بندی هیپوکامپ}

هیپوکامپ\LTRfootnote{Hippocampus} ساختمان عصبی خمیده‌ای است در مغز که در میانه‌ی بطن‌های طرفی مغز قرار دارد. هیپوکامپ در اعماق لوب گیجگاهی جای گرفته‌است و از دو شاخ منحنی‌وار تشکیل شده‌است که از بخش‌های مهم مغز پستانداران است. حافظه افرادی که هیپوکامپ آنان آسیب دیده یا با جراحی برداشته شده، دچار اختلال جدی می‌شود. هیپوکامپ تثبیت‌کننده میان حافظه کوتاه‌مدت و بلندمدت است و مغز قدامی را از آزموده‌های گذشته ما آگاه می‌کند. این مجموعه خاطرات گذشته را به شکل کوتاه‌مدت یا درازمدت حفظ می‌کند \مرجع{martin2003lymbic}. 

بنابراین این عضو نیز از ساختارهای مهم و حیاتی است و لازم است در طی پرتو درمانی تومورهای مغزی این ساختار نیز به عنوان ساختار در ریسک، قطعه‌بندی شود و از آسیب به آن جلوگیری شود. مجموعه داده‌ی عمومی قطعه‌بندی تصاویر پزشکی Decathlon \مرجع{simpson2019large} شامل 260 تصویر سه‌بعدی ام‌آرای T1-weighted است که دو بخش سر و بدن هیپوکامپ را به صورت دستی و به عنوان قطعه‌بندی مطلوب ارایه داده است. ابعاد تصاویر در بازه‌ی 
$(31\sim43)*(40\sim59)*(24\sim47)$
و با فاصله‌ی واکسلی یک میلی‌متر هستند. 

برای پیش‌پردازش این دادگان ابتدا برای همسان‌سازی اندازه، تمام ابعاد با اضافه کردن صفر\LTRfootnote{Zero Padding} به اندازه‌ی $48*64*48$ تغییر داده شد. در گام‌ بعدی برای نرمالیزه کردن تصاویر از هیستوگرام تصویر کمک گرفته شد و با تقسیم مقادیر به شدت 95 درصد هیستوگرام تجمعی این نرمال‌سازی صورت گرفت. دلیل این‌کار و استفاده نکردن از مقدار بیشینه مقاوم کردن نرمال‌سازی نسبت به نویزهای با مقدار بالا در تصاویر ام‌آرآی است. در گام آخر پیش‌پردازش برچسب‌های دو قسمت هیپوکامپ، برای ایجاد یک برچسب در قطعه‌بندی با یکدیگر ترکیب شدند (در ادامه خواهیم دید از این برچسب‌ها برای یک تابع هزینه‌ی خاص استفاده شده است و لازم است ساختار پیوست باشد). شگل ~\رجوع{شکل:هیپوپیری} تصویر قبل (الف) و بعد از پیش‌پردازش (ب) هیپوکامپ را با قطعه‌بندی متناظر نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04hippocampusprep.png}{16cm}
\شرح{نمونه‌ی یک تصویر ام آرآی هیپوکامپ در نماهای مختلف با قطعه‌بندی (الف) تصویر اصلی (ب) تصویر پس از پیش‌پردازش}
\برچسب{شکل:هیپوپیری}
\پایان{شکل}

\زیرقسمت{آموزش قطعه‌بندی با استفاده از متد چگالش دانش}

با ظهور روش‌های یادگیری عمیق و افزایش سرعت پردازش‌گرها،امکان تعریف مدل‌های بسیار پیچیده با تعداد پارامترهای قابل یادگیری بسیار زیاد فراهم گردید. بنابراین، این مدل‌های پیچیده، توانایی استخراج ویژگی‌های سطح بالا و در نتیجه مستعد اخذ دقت بالاتر هستند. اما از طرفی گران بودن دستگاه‌های با قدرت پردازش‌ بالا و نیز محدودیت استفاده‌ی آن‌ها توجهات را به سمت انتقال دانش از مدل‌های پیچیده به سمت مدل‌های ساده‌تر جلب کرد. این کار باعث می‌شود، ویژگی‌هایی که مدل بسیار پیچیده توانایی استخراج‌ آن‌ها را دارد به مدل ساده‌تر منتقل شود و علاوه بر بدست آوردن دقت نزدیک به مدل پیچیده، هزینه‌های محاسباتی نیز کمتر شود \مرجع{cheng2018model}.

یکی از این روش‌ها چگالش دانش\LTRfootnote{Knowledge Distillation} از مدل پیچیده (آموزگار\LTRfootnote{Teacher}) به مدل ساده (دانش‌آموز\LTRfootnote{Student}) است که در ادامه قصد داریم این روش را در قطعه‌بندی ساختارهای در ریسک استفاده کنیم و مدل‌های ساده‌تر برای این قطعه‌بندی را به دقت‌های بالاتر بدون تغییر هزینه‌های محاسباتی برسانیم.

\زیرزیرقسمت{چگالش دانش}

چگالش دانش از یادگیری انسان اقتباس شده‌است که یک آموزگار که در یک موضوع مسلط است به دانش‌آموز آموزش می‌دهد. بنابراین چارچوب چگالش دانش را می‌توان شامل یک یا چند مدل بزرگ از پیش آموزش دیده شده و یک مدل ضعیف تعریف کرد که ایده‌ی اصلی آن آموزش مدل ضعیف‌تر (دانش‌آموز) با نظارت مدل پیچیده (آموزگار) برای رسیدن به دقت قابل مقایسه با آموزگار است.

سیگنال نظارتی که از مدل آموزگار به دانش‌آموز می رسد را	 \مهم{دانش} می‌نامیم که قبلا توسط آموزگار یادگرفته شده است و دانش‌آموز سعی در تقلید رفتار آموزگار در یادگیری دانش را دارد. به عنوان مثال در یک مساله‌ی طبقه‌بندی تصاویر، لاجیت‌ها\LTRfootnote{Logits}(خروجی آخرین لایه در شبکه‌های عصبی عمیق) به عنوان حامل‌های دانش از مدل آموزگار به مدل دانش‌آموز استفاده می‌شود که این دانش توسط برچسب‌های صحیح مطلوب تامین نمی شود. برای فهم بیشتر این مطلب، فرض کنید یک مساله‌ی طبقه‌بندی بین چهار طبقه‌ی گاو، سگ، گربه و ماشین وجود دارد. در نهایت بعد از آموزش یک مدل بر روی این دادگان دو نوع خروجی نرم و سخت\LTRfootnote{Soft and Hard Targets} در دسترس است. خروجی‌های سخت مربوط به برچسب‌های مطلوب است و خروجی‌های نرم مربوط به احتمالات پیش‌بینی شده برای هر طبقه، توسط مدل است. با توجه به شکل ~\رجوع{شکل:هاردسافتتارگت} و مقادیری که برای پیش‌بینی یک تصویر به عنوان سگ شده است می‌توان دریافت علاوه بر پیش‌بینی با احتمال بالا برای کلاس صحیح، میزان مشابهت با کلاس‌های دیگر نیز در خروجی‌های نرم وجود دارد. به عنوان مثال به علت شباهت زیاد سگ و گربه مقدار $0.1$ برای این شباهت بدست آمده است اما در مقایسه با کلاس‌های ماشین و گاو، کلاس گاو نسبت به کلاس ماشین به علت حیوان و چهارپا بودن هر دو (سگ و گاو) احتمال بیشتری نسبت داده شده است. بنابراین در این نوع خروجی‌ها اطلاعات بیشتری نسبت به خروجی‌های سخت وجود دارد \مرجع{liu2018improving}.

\شروع{شکل}[H]
\centerimg{04softvshardtargets.png}{8cm}
\شرح{خروجی‌های سخت و نرم برای طبقه‌بندی چهار کلاس \مرجع{liu2018improving}}
\برچسب{شکل:هاردسافتتارگت}
\پایان{شکل}

برای بدست آوردن احتمال حضور در iامین طبقه، لاجیت‌ها را از یک تابع فعالیت Softmax عبور می‌دهند تا این احتمال با مقدار $p_i$ مشخص شود. معادله‌ی ~\رجوع{softmax} تابع فعالیت Softmax را نشان می‌دهد.

\begin{alignat}{5}
	p_i = \frac{exp(z_i)}{\Sigma_j exp(z_j)}    \label{softmax} 
\end{alignat}

برای استخراج خروجی‌های نرم، با اضافه کردن یک فاکتور دما\LTRfootnote{Temperature Factor}(T) به معادله‌ی Softmax می‌توان این خروجی‌ها را استخراج کرد. با افزایش مقدار فاکتور دما میزان نرم‌ شدن خروجی‌ها بیشتر می‌شود و با کاهش آن مقدار خروجی‌ها به خروجی‌های سخت نزدیک‌تر می‌شود. بنابراین با این فاکتر می‌توان میزان اهمیت هر برچسب را نرم را کنترل کرد. معادله‌ی زیر چگونگی اعمال فاکتور دما در معادله‌ی Softmax را نشان می‌دهد.

\begin{alignat}{5}
	p_i = \frac{exp(z_i/T)}{\Sigma_j exp(z_j/T)}    \label{softsoftmax} 
\end{alignat}

خروجی‌های نرم در مدل‌های آموزگار و دانش‌آموز و نیز خروجی‌های مطلوب\LTRfootnote{Ground Truth} نقش مهمی در آموزش دانش‌آموز دارند و با داشتن این مقادیر می‌توان تابع‌ هزینه‌ی مربوط به چگالش دانش\LTRfootnote{Distillation Loss} و 
تابع هزینه‌ی دانش‌آموز\LTRfootnote{Student Loss} را تعریف نمود. تابع هزینه‌ی چگالش دانش را می‌توان به صورت زیر میان لاجیت‌های آموز‌گار و دانش‌آموز به صورت یک تابع هزینه‌ی \lr{Cross-Entropy} نوشت. 

\begin{alignat}{5}
	L_D(p(z_t,T), p(z_s,T)) = -\Sigma_i p_i(z_{ti},T) \log(p_i(z_{si},T))    \label{ld} 
\end{alignat}

که در آن $z_t$ و $z_s$ به ترتیب، لاجیت‌های آموزگار و دانش‌آموز هستند. گرادیان تابع‌ هزینه‌ی چگالش نسبت به لاجیت‌های دانش‌آموز می‌تواند به صورت زیر محاسبه گردد.

\begin{alignat}{5}
	\frac{\partial L_D(p(z_t,T), p(z_s,T))}{\partial z_{si}} = \frac{p(z_t,T) - p(z_s,T)}{T}    
	= \frac{1}{T}(\frac{exp(z_{si}/T)}{\Sigma_j exp(z_{sj}/T)}  -\frac{exp(z_{ti}/T)}{\Sigma_j exp(z_{tj}/T)}) \label{gradld}
\end{alignat}

اگر فاکتور دما (T) بسیار بزرگتر از مقدار لاجیت‌ها باشد آنگاه با استفاده از بسط تیلور معادله‌ی ~\رجوع{gradld} را می‌توان به صورت زیر بازنویسی نمود.
\begin{alignat}{5}
	\frac{\partial L_D(p(z_t,T), p(z_s,T))}{\partial z_{si}} = \frac{1}{T}(\frac{1+\frac{z_{si}}{T}}{N+\Sigma_j \frac{z_{sj}}{T}} - \frac{1+\frac{z_{ti}}{T}}{N+\Sigma_j \frac{z_{tj}}{T}})    
	\label{gradsimplelddd}
\end{alignat}

با فرض میانگین صفر بودن لاجیت‌های آموزگار و دانش آموز معادله‌ی فوق را می‌توان ساده‌تر نمود.
\begin{alignat}{5}
	\frac{\partial L_D(p(z_t,T), p(z_s,T))}{\partial z_{si}} = \frac{1}{NT^2}(z_{si}-z_{ti})    
	 \label{gradsimpleld}
\end{alignat}

بنابراین با توجه به معادله‌ی ~\رجوع{gradsimpleld} هدف تابع هزینه‌ی چگالش، کمینه کردن فاصله‌ی میان لاجیت‌های آموزگار و شاگرد است \مرجع{hinton2015distilling}. تابع هزینه‌ی دانش‌آموز را نیز می‌توان به صورت \lr{Cross-Entropy} بین خروجی مطلوب و لاجیت‌های دانش‌آموز به صورت زیر نوشت.
\begin{alignat}{5}
	L_S(y, p(z_s,T)) = -\Sigma_i y \log(p_i(z_{si},T))    \label{ls} 
\end{alignat}

در نهایت، با کنار هم قرار دادن تابع‌ هزینه‌‌ی چگالش و تابع هزینه‌ی دانش‌آموز، تابع هزینه‌ی کلی برای چارچوب چگالش دانش از آموزگار به شاگرد طبق معادله‌ی ~\رجوع{losstotal} تشکیل می‌شود که با ضریب $\alpha$ مجموع این دو تابع به صورت وزن‌دار محاسبه می‌شود.

\begin{alignat}{5}
	L_{TotalDistillation} = \alpha L_D(p(z_t,T), p(z_s,T)) + (1-\alpha) L_S(y, p(z_s,T))   \label{losstotal} 
\end{alignat}

فاکتور دمای T در تابع هزینه‌ی دانش‌آموز برابر با یک و در تابع‌ هزینه‌ی چگالش برابر یا بزرگتر از یک در نظر گرفته می‌شود. بنابراین به صورت کلی، چارچوب چگالش دانش را می‌توان به صورت شکل ~\رجوع{شکل:نالجدیس} بیان نمود که در آن، آموزگار یک مدل از پیش آموزش دیده شده‌است و دانش‌آموز به واسطه‌ی خروجی‌های نرم و مقادیر مطلوب که دو تابع هزینه را تشکیل می‌دهند، آموزش داده می‌شود \مرجع{gou2021knowledge}.

\شروع{شکل}[H]
\centerimg{04knowledgedistillationframework.png}{15cm}
\شرح{عملکرد انتقال دانش توسط چارچوب چگالش دانش \مرجع{gou2021knowledge}}
\برچسب{شکل:نالجدیس}
\پایان{شکل}


\زیرزیرقسمت{معماری شبکه‌های قطعه‌بند}

همانطور که پیش‌تر اشاره گردید، در این قسمت، قصد داریم با استفاده از چارچوب چگالش دانش در بحث قطعه‌بندی ساختارهای در ریسک استفاده کنیم. برای این‌کار نیاز به مدل‌های آموزگار و دانش‌آموز است که در ابتدا یک مدل پیچیده با توانایی بالا بر روی دادگان به عنوان آموزگار آموزش می‌بیند و سپس مدل ساده‌تر (دانش‌آموز) با استفاده از چارچوب چگالش دانش، از دانش مدل آموزگار برای بهبود عملکرد بهره می‌برد.

برای مدل آموزگار از معماری یک UNet پیچیده با تعداد پارامتر‌های قابل آموزش بالا و روش‌های تعمیم‌پذیری استفاده گردید. معماری این شبکه‌ی تمام کانوولوشنی عمیق در شکل ~\رجوع{شکل:تیچریونت} قابل مشاهده است.

\شروع{شکل}[H]
\centerimg{04teacherUnet.png}{15cm}
\شرح{معماری شبکه‌ی UNet پیچیده، استفاده شده به عنوان آموزگار}
\برچسب{شکل:تیچریونت}
\پایان{شکل}

تفاوت این معماری با معماری UNet اولیه‌ی معرفی شده در \مرجع{ronneberger2015u} استفاده از ابزارهای افزایش قدرت تعمیم‌پذیری مانند dropout و Batch-normalization پس از لایه‌های کانوولوشنی است. همچنین برای افزایش وسعت دید شبکه و استخراج ویژگی‌ها عمیق‌تر  و کلی‌تر از سطح تصویر، اندازه‌ی فیلتر‌های کانوولوشنی برابر با $13*13$ تعریف گردید.

در ادامه، برای تعریف مدل دانش‌آموز، از دو معماری ساده‌تر استفاده شد. دانش‌آموز اول یک مدل UNet بسیار ساده‌تر با تعداد پارامترهای بسیار کمتر از UNet پیچیده‌ی آموزگار و دانش‌آموز دوم بک معماری بر اساس بلوک‌های Residual است. شکل ~\رجوع{شکل:اسوانیونت} معماری یک UNet ساده شده را به عنوان دانش‌آموز اول نشان می‌دهد که در مقایسه با معماری آموزگار در شکل ~\رجوع{شکل:تیچریونت}، ساختار ساده‌تری دارد که عبارتند از: کاهش ابعاد فیلترهای کانوولوشنی به $3*3$ و در نتیجه کاهش وسعت دید هر فیلتر، استفاده نکردن از روش‌های تعمیم‌پذیری dropout و Batch-normalization و در نهایت کاهش تعداد ویژگی‌های استخراج شده به نصف، نسبت به UNet پیچیده. 

\شروع{شکل}[H]
\centerimg{04simpleUnet.png}{10cm}
\شرح{معماری شبکه‌ی UNet ساده، استفاده شده به عنوان دانش‌آموز اول}
\برچسب{شکل:اسوانیونت}
\پایان{شکل}

شکل ~\رجوع{شکل:استو}، معماری دانش‌آموز دوم را که بر اساس بلوک‌های Residual طراحی شده است را نشان می‌دهد. که دوباره، نسبت به مدل آموزگار، دارای تعداد ویژگی نصف در هر لایه‌ است منتها برای آموزش بهتر آن از روش Batch-normalization نیز استفاده شده است.

\شروع{شکل}[H]
\centerimg{04Resnetstudent.png}{15cm}
\شرح{معماری شبکه‌ی پیشنهادی بر اساس بلوک‌های Residual، استفاده شده به عنوان دانش‌آموز دوم}
\برچسب{شکل:استو}
\پایان{شکل}

\زیرزیرقسمت{آموزش شبکه‌های قطعه‌بند}

برای آموزش شبکه‌های معرفی شده در بالا، از مجموعه دادگان ساختارهای در ریسک قفسه‌ی سینه‌ی SegTHOR معرفی شده در بخش ~\رجوع{segthor} استفاده گردید. همانظور که گفته شد، این مجموعه داده قطعه‌بندی چهار ساختار در ریسک را در 40 تصویر سی‌تی اسکن قفسه‌ی سینه تهیه نموده است. بنابراین برای قطعه‌بندی این چهار ساختار، همانطور که در شکل‌های ~\رجوع{شکل:تیچریونت} و ~\رجوع{شکل:اسوانیونت} نشان داده شده است، در لایه‌ی خروجی این معماری‌ها چهار خروجی گرفته شده است که هر یک از آن‌ها مسوول قطعه‌بندی یکی از ساختارهاست.

برای آموزش بر اساس چارچوب چگالش دانش لازم است تابع فعالیت خروجی برای کلاس‌ها Softmax باشد که با توجه به چهار ساختار در ریسک قلب، آئورت، نای و مری یک کلاس پس‌زمینه نیز به این چهار کلاس اضافه می‌شود تا بتوان از تابع هزینه‌ی \lr{Cross-Entropy} استفاده نمود. اما با آزمایش‌های اولیه‌ای که صورت گرفت، ملاحظه گردید در نظر نگرفتن پس‌زمینه به عنوان یک کلاس مجزا و فقط قطعه‌بندی چهار ساختار در ریسک، دقت بالاتری را بوجود می‌آورد بنابراین معماری شبکه‌ها برای ایجاد خروجی، تنها برای ساختارهای در ریسک طراحی گردید.

در این حالت، مشکلی که بوجود می‌آید عدم توانایی در استفاده از تابع هزینه‌ی \lr{Cross-Entropy} است زیرا دیگر طبقه‌ی اضافی پس زمینه وجود ندارد. برای حل این مشکل از تابع هزینه‌ی \lr{Binary Cross-Entropy} استفاده شد که به صورت زیر تعریف می‌شود:

\begin{alignat}{5}
	BCE = -\frac{1}{N}\sum_{i=1}^N y_i \log(p_i)+(1-y_i)  \log(1-p_i)  \label{bce} 
\end{alignat}

که در آن $y_i$ خروجی مطلوب که صفر یا یک است و $p_i$ خروجی پیش‌بینی شده است که در بازه‌ی صفر و یک است. فرض کنید اگر خروجی مطلوب برابر با صفر باشد، ترم اول در معادله‌ی ~\رجوع{bce} حذف می‌شود و ترم دوم سعی می‌کند با کمینه‌کردن مقدار هزینه، مقدار پیش‌بینی شده را به صفر نزدیک کند. بنابراین با صفر بودن دایمی یکی از کلاس‌ها با استفاده از این تابع هزینه، مشکلی در یادگیری بوجود نمی‌آید و با حل کردن این مساله‌ی بهینه‌سازی برای کلاس پس‌زمینه، مقدار $0.25$ در هر یک از نقشه‌های احتمال چهار ساختار در ریسک، بدست می‌آید.

\زیرزیرقسمت{نتایج}

\قسمت{روش‌های پیشنهادی قطعه‌بندی تومور}

\زیرقسمت{معرفی دادگان و پیش‌ پردازش}