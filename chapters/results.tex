
\فصل{روش‌های پیشنهادی و نتایج}

\قسمت{مقدمه}
در این فصل به معرفی روش‌های ارایه شده در این مطالعه، برای قطعه‌بندی ساختارهای در ریسک و تومور با استفاده از شبکه‌های عصبی عمیق پرداخته خواهد شد. مطالعه شامل دو بخش است که در بخش اول به قطعه‌بندی ساختارهای در ریسک، چالش‌های موجود، راهکارهای پیشنهادی و ارزیابی روش‌های پیشنهادی پرداخه می‌شود و در بخش دوم مطالعه همین روند برای قطعه‌بندی تومور طی خواهد شد.

\قسمت{روش‌های پیشنهادی قطعه‌بندی ساختارهای در ریسک}

همانطور که در فصل اول اشاره گردید، ضرورت قطعه‌بندی ساختارهای در ریسک، در تهیه‌ی نقشه‌ی درمان برای رادیوتراپی است. این نقشه‌ی درمان کمک می‌کند تا به بافت‌های سالم کنار تومور (ساختارهای در ریسک) کمترین آسیب و تومور بیشترین دوز پرتو را دریافت کند. بنابراین لازم است این ساختارها با دقت  و سرعت بالا قطعه‌بندی شوند و در روند درمان قرار گیرند. این قطعه‌بندی به صورت دستی زمان‌بر، پرهزینه (هزینه‌ی استخدام شخص متخصص)، خسته‌کننده و متغیر بر اساس دیدگاه هر متخصص است.

بنابراین برای تسریع این روند و حل مشکلات ذکر شده، روش‌های خودکار و نیمه‌-خودکار فراوانی ارایه گردید که در فصل قبل به بررسی هر یک پرداخته شد. ملاحظه گردید که الگوریتم‌های کلاسیک ارایه شده، در اکثر موارد توانایی قطعه‌بندی چند ساختار به صورت همزمان را ندارند و علاوه بر آن در اکثر موارد نیاز به یک مقدار (کانتور) اولیه دارند که این کاستی‌ها در کنار دقت پایین باعث محبوبیت پایین این روش‌ها شده است.

در دست دیگر، روش‌های قطعه‌بندی مبتنی بر یادگیری عمیق بررسی شد که با آموزش این مدل‌ها بر اساس یک مجموعه داده، علاوه بر سرعت و دقت بالا نسبت به روش‌های قطعه‌بندی کلاسیک و دستی، امکان قطعه‌بندی خودکار چند ساختار نیز وجود دارد. بنابراین شاهد استفاده‌ی روز افزون این روش‌ها و افزایش اعتبار و محبوبیت آن‌ها برای استفاده در نرم‌افزارهای قطعه‌بندی ساختارهای در ریسک در مراکز پرتو درمانی هستیم.

در این بخش به توضیح روش‌های پیشنهادی برای قطعه‌بندی ساختارهای در ریسک  بر اساس مدل‌های یادگیری عمیق پرداخته می‌شود. ابتدا مجموعه دادگان و پیش‌پردازش‌های انجام شده، توضیح داده خواهد شد و در ادامه، روش‌های پیشنهادی معرفی می‌شوند و نتایج این روش‌ها بررسی خواهد شد.

\زیرقسمت{معرفی مجموعه دادگان و پیش‌‌پردازش}
\label{segthor}
در این قسمت، دو مجموعه داده‌ برای قطعه‌بندی ساختارهای در ریسک معرفی می‌شود که در ادامه به بررسی هر یک و پیش‌پردازش‌های انجام شده، پرداخته خواهد شد.
\زیرزیرقسمت{مجموعه داده‌ی SegTHOR}

مجموعه داده‌ی SegTHOR شامل قطعه‌بندی چهار ساختار در ریسک قفسه‌ی سینه در تصاویر سی‌تی اسکن است\LTRfootnote{َSegmentation of THoracic Organs at Risk}. این مجموعه تصاویر، از 40 بیمار دارای سرطان ریه و مری گرفته شده است و شامل قطعه‌بندی دستی چهار ساختار در ریسک قلب\LTRfootnote{Heart} ،آئورت\LTRfootnote{Aorta}، نای\LTRfootnote{Trachea} و مری\LTRfootnote{Esophagus} است که به عنوان استاندارد مطلوب\LTRfootnote{Ground Truth} برای آموزش شبکه‌های عصبی تهیه شده است.

این مجموعه داده برای قطعه‌بندی ساختارهای در ریسک به دلیل تغییر شکل ساختار‌های در ریسک (مانند مری) در هر شخص و نیز اختلاف شدت پایین در میان بافت‌های نرم در تصاویر سی‌تی اسکن، بسیار پر چالش است \مرجع{lambert2020segthor}. شکل ~\رجوع{شکل:سگتوراوریجینال} یک نمونه از تصاویر اولیه‌ی موجود در این مجموعه داده را نشان می‌دهد که به علت کمبود اختلاف شده‌ها به سختی بدن بیمار قابل تشخیص است. 

\شروع{شکل}[H]
\centerimg{04segthorrawimg.png}{8cm}
\شرح{نمونه‌ی یک تصویر سی‌تی اسکن از مجموعه داده‌ی SegTHOR در سه نمای اکسیال، کرونال و سجیتال}
\برچسب{شکل:سگتوراوریجینال}
\پایان{شکل}

این تصاویر دارای ابعاد اولیه‌ی $512*512*(150\sim 284)$ و با فاصله‌ی واکسلی\LTRfootnote{Voxel Spacing}،
 $(0.97\sim1.36)*(0.97\sim1.36)*(2\sim2.5)$
 در راستاهای x، y و z است که حاکی از متفاوت بودن دستگاه‌های تصویر برداری در این مجموعه دادگان است. بنابراین لازم است برای تمایز هرچه بهتر بافت‌ها و یکسان‌سازی تصاویر دستگاه‌های مختلف در جهت آموزش شبکه‌ی عصبی عمیق، یک پیش‌پردازش مناسب لازم است.
 
اولین گام در پیش‌پردازش یکسان‌سازی فاصله‌ی واکسلی به $0.97*0.97*2$ است که با استفاده از درون‌یابی برای تصویر اصلی و تصویر قطعه‌بندی متناظر این کار انجام گردید و تا حدی تفاوت تصویربرداری در بین دستگاه‌های مختلف یکسان‌سازی شد. در گام دوم، ناحیه‌ی شامل ساختارهایی که باید قطعه‌بندی شوند از تصویر اصلی استخراج گردید و سایر نواحی مانند قسمت‌های شکمی و سر و گردن حذف شد. در گام سوم مقدار \LTRfootnote{Hounsfield Unit}HU در تصاویر اصلی که در بازه‌ی $(-1000\sim 13000)$ بود مقادیری که در خارج از بازه‌ی $(-800\sim 400)$ بود، به سقف و کف این بازه تبدیل داده شد و مقادیر این بازه در تصاویر اصلی بین صفر و یک نرمالیزه گردید. در نهایت فضاهای خالی اطراف تصویر به مرکزیت آن بریده شد و ابعاد $512*512$ به $384*288$ کاهش یافت. شکل زیر تصویر نشان داده شده در شکل ~\رجوع{شکل:سگتوراوریجینال} را بعد از پیش‌پردازش نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04segthorprepimg.png}{8cm}
\شرح{نمونه‌ی یک تصویر سی‌تی اسکن از مجموعه داده‌ی SegTHOR بعد از پیش‌پردازش، در سه نمای اکسیال، کرونال و سجیتال}
\برچسب{شکل:سگتورپیریپراسسد}
\پایان{شکل}

برای حفظ تناظر یک به یک بین تصاویر اصلی و قطعه‌بندی استاندارد، تغییرات و پردازش‌های اعمال شده در بالا، مطابق با ماهیت برچسب‌ها، اعمال گردید تا دوباره ماسک‌های قطعه‌بندی شده در روند آموزش قابل استفاده گردند. شکل ~\رجوع{شکل:سگتورویتلیبل} نمونه‌ای از تصویر اصلی همراه با برچسب‌های چهار ساختار در ریسک را نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04segthorprepimg_withlabel.png}{8cm}
\شرح{نمونه‌ی یک تصویر سی‌تی اسکن از مجموعه داده‌ی SegTHOR بعد از پیش‌پردازش همراه با برچسب ساختار در ریسک، در چهار نمای اکسیال، کرونال، سجیتال و سه‌بعدی}
\برچسب{شکل:سگتورویتلیبل}
\پایان{شکل}

برای آموزش شبکه (شبکه‌های به صورت دوبعدی آموزش داده می‌شوند) و افزایش قدرت تعمیم‌پذیری آن از روش افزایش مجموعه دادگان استفاده شد که در اینجا به معرفی جزییات آن پرداخته می‌شود. هر تصویر به صورت دو بعدی و از نمای اکسیال از تصویر سه‌بعدی پردازش شده با برچسب ساختارهای در ریسک متناظر استخراج می‌گردد و با اعمال تبدیلات دوران تصادفی بین زاویه‌ی $(-5,5)$ درجه، بزرگ‌نمایی و کوچک‌نمایی با ضرایب $1.1$ و $0.9$ و قرینه‌ی افقی تعداد دادگان آموزش افزایش می‌یابد. در شکل ~\رجوع{شکل:سگتوراگمنت} (الف) تصویر دوبعدی اصلی با برچسب‌ متناظر نشان داده شده است و در (ب) قرینه‌ی افقی و بزرگ‌نمایی آن با ضریب $1.1$ به تصویر کشیده شده‌است.

\شروع{شکل}[H]
\centerimg{04segthoraugmentation.png}{8cm}
\شرح{نمونه‌ی یک تصویر در نمای اکسیال با برچسب ساختار در ریسک متناظر (الف) تصویر اصلی (ب) تصویر تبدیل یافته برای افزایش دادگان}
\برچسب{شکل:سگتوراگمنت}
\پایان{شکل}

\زیرزیرقسمت{مجموعه داده‌ی قطعه‌بندی هیپوکامپ}

هیپوکامپ\LTRfootnote{Hippocampus} ساختمان عصبی خمیده‌ای است در مغز که در میانه‌ی بطن‌های طرفی مغز قرار دارد. هیپوکامپ در اعماق لوب گیجگاهی جای گرفته‌است و از دو شاخ منحنی‌وار تشکیل شده‌است که از بخش‌های مهم مغز پستانداران است. حافظه افرادی که هیپوکامپ آنان آسیب دیده یا با جراحی برداشته شده، دچار اختلال جدی می‌شود. هیپوکامپ تثبیت‌کننده میان حافظه کوتاه‌مدت و بلندمدت است و مغز قدامی را از آزموده‌های گذشته ما آگاه می‌کند. این مجموعه خاطرات گذشته را به شکل کوتاه‌مدت یا درازمدت حفظ می‌کند \مرجع{martin2003lymbic}. 

بنابراین این عضو نیز از ساختارهای مهم و حیاتی است و لازم است در طی پرتو درمانی تومورهای مغزی این ساختار نیز به عنوان ساختار در ریسک، قطعه‌بندی شود و از آسیب به آن جلوگیری شود. مجموعه داده‌ی عمومی قطعه‌بندی تصاویر پزشکی Decathlon \مرجع{simpson2019large} شامل 260 تصویر سه‌بعدی ام‌آرای T1-weighted است که دو بخش سر و بدن هیپوکامپ را به صورت دستی و به عنوان قطعه‌بندی مطلوب ارایه داده است. ابعاد تصاویر در بازه‌ی 
$(31\sim43)*(40\sim59)*(24\sim47)$
و با فاصله‌ی واکسلی یک میلی‌متر هستند. 

برای پیش‌پردازش این دادگان ابتدا برای همسان‌سازی اندازه، تمام ابعاد با اضافه کردن صفر\LTRfootnote{Zero Padding} به اندازه‌ی $48*64*48$ تغییر داده شد. در گام‌ بعدی برای نرمالیزه کردن تصاویر از هیستوگرام تصویر کمک گرفته شد و با تقسیم مقادیر به شدت 95 درصد هیستوگرام تجمعی این نرمال‌سازی صورت گرفت. دلیل این‌کار و استفاده نکردن از مقدار بیشینه مقاوم کردن نرمال‌سازی نسبت به نویزهای با مقدار بالا در تصاویر ام‌آرآی است. در گام آخر پیش‌پردازش برچسب‌های دو قسمت هیپوکامپ، برای ایجاد یک برچسب در قطعه‌بندی با یکدیگر ترکیب شدند (در ادامه خواهیم دید از این برچسب‌ها برای یک تابع هزینه‌ی خاص استفاده شده است و لازم است ساختار پیوست باشد). شگل ~\رجوع{شکل:هیپوپیری} تصویر قبل (الف) و بعد از پیش‌پردازش (ب) هیپوکامپ را با قطعه‌بندی متناظر نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04hippocampusprep.png}{16cm}
\شرح{نمونه‌ی یک تصویر ام آرآی هیپوکامپ در نماهای مختلف با قطعه‌بندی (الف) تصویر اصلی (ب) تصویر پس از پیش‌پردازش}
\برچسب{شکل:هیپوپیری}
\پایان{شکل}

\زیرقسمت{آموزش قطعه‌بندی با استفاده از متد چگالش دانش}

با ظهور روش‌های یادگیری عمیق و افزایش سرعت پردازش‌گرها،امکان تعریف مدل‌های بسیار پیچیده با تعداد پارامترهای قابل یادگیری بسیار زیاد فراهم گردید. بنابراین، این مدل‌های پیچیده، توانایی استخراج ویژگی‌های سطح بالا و در نتیجه مستعد اخذ دقت بالاتر هستند. اما از طرفی گران بودن دستگاه‌های با قدرت پردازش‌ بالا و نیز محدودیت استفاده‌ی آن‌ها توجهات را به سمت انتقال دانش از مدل‌های پیچیده به سمت مدل‌های ساده‌تر جلب کرد. این کار باعث می‌شود، ویژگی‌هایی که مدل بسیار پیچیده توانایی استخراج‌ آن‌ها را دارد به مدل ساده‌تر منتقل شود و علاوه بر بدست آوردن دقت نزدیک به مدل پیچیده، هزینه‌های محاسباتی نیز کمتر شود \مرجع{cheng2018model}.

یکی از این روش‌ها چگالش دانش\LTRfootnote{Knowledge Distillation} از مدل پیچیده (آموزگار\LTRfootnote{Teacher}) به مدل ساده (دانش‌آموز\LTRfootnote{Student}) است که در ادامه قصد داریم این روش را در قطعه‌بندی ساختارهای در ریسک استفاده کنیم و مدل‌های ساده‌تر برای این قطعه‌بندی را به دقت‌های بالاتر بدون تغییر هزینه‌های محاسباتی برسانیم.

\زیرزیرقسمت{چگالش دانش}

چگالش دانش از یادگیری انسان اقتباس شده‌است که یک آموزگار که در یک موضوع مسلط است به دانش‌آموز آموزش می‌دهد. بنابراین چارچوب چگالش دانش را می‌توان شامل یک یا چند مدل بزرگ از پیش آموزش دیده شده و یک مدل ضعیف تعریف کرد که ایده‌ی اصلی آن آموزش مدل ضعیف‌تر (دانش‌آموز) با نظارت مدل پیچیده (آموزگار) برای رسیدن به دقت قابل مقایسه با آموزگار است.

سیگنال نظارتی که از مدل آموزگار به دانش‌آموز می رسد را	 \مهم{دانش} می‌نامیم که قبلا توسط آموزگار یادگرفته شده است و دانش‌آموز سعی در تقلید رفتار آموزگار در یادگیری دانش را دارد. به عنوان مثال در یک مساله‌ی طبقه‌بندی تصاویر، لاجیت‌ها\LTRfootnote{Logits}(خروجی آخرین لایه در شبکه‌های عصبی عمیق) به عنوان حامل‌های دانش از مدل آموزگار به مدل دانش‌آموز استفاده می‌شود که این دانش توسط برچسب‌های صحیح مطلوب تامین نمی شود. برای فهم بیشتر این مطلب، فرض کنید یک مساله‌ی طبقه‌بندی بین چهار طبقه‌ی گاو، سگ، گربه و ماشین وجود دارد. در نهایت بعد از آموزش یک مدل بر روی این دادگان دو نوع خروجی نرم و سخت\LTRfootnote{Soft and Hard Targets} در دسترس است. خروجی‌های سخت مربوط به برچسب‌های مطلوب است و خروجی‌های نرم مربوط به احتمالات پیش‌بینی شده برای هر طبقه، توسط مدل است. با توجه به شکل ~\رجوع{شکل:هاردسافتتارگت} و مقادیری که برای پیش‌بینی یک تصویر به عنوان سگ شده است می‌توان دریافت علاوه بر پیش‌بینی با احتمال بالا برای کلاس صحیح، میزان مشابهت با کلاس‌های دیگر نیز در خروجی‌های نرم وجود دارد. به عنوان مثال به علت شباهت زیاد سگ و گربه مقدار $0.1$ برای این شباهت بدست آمده است اما در مقایسه با کلاس‌های ماشین و گاو، کلاس گاو نسبت به کلاس ماشین به علت حیوان و چهارپا بودن هر دو (سگ و گاو) احتمال بیشتری نسبت داده شده است. بنابراین در این نوع خروجی‌ها اطلاعات بیشتری نسبت به خروجی‌های سخت وجود دارد \مرجع{liu2018improving}.

\شروع{شکل}[H]
\centerimg{04softvshardtargets.png}{8cm}
\شرح{خروجی‌های سخت و نرم برای طبقه‌بندی چهار کلاس \مرجع{liu2018improving}}
\برچسب{شکل:هاردسافتتارگت}
\پایان{شکل}

برای بدست آوردن احتمال حضور در iامین طبقه، لاجیت‌ها را از یک تابع فعالیت Softmax عبور می‌دهند تا این احتمال با مقدار $p_i$ مشخص شود. معادله‌ی ~\رجوع{softmax} تابع فعالیت Softmax را نشان می‌دهد.

\begin{alignat}{5}
	p_i = \frac{exp(z_i)}{\Sigma_j exp(z_j)}    \label{softmax} 
\end{alignat}

برای استخراج خروجی‌های نرم، با اضافه کردن یک فاکتور دما\LTRfootnote{Temperature Factor}(T) به معادله‌ی Softmax می‌توان این خروجی‌ها را استخراج کرد. با افزایش مقدار فاکتور دما میزان نرم‌ شدن خروجی‌ها بیشتر می‌شود و با کاهش آن مقدار خروجی‌ها به خروجی‌های سخت نزدیک‌تر می‌شود. بنابراین با این فاکتر می‌توان میزان اهمیت هر برچسب را نرم را کنترل کرد. معادله‌ی زیر چگونگی اعمال فاکتور دما در معادله‌ی Softmax را نشان می‌دهد.

\begin{alignat}{5}
	p_i = \frac{exp(z_i/T)}{\Sigma_j exp(z_j/T)}    \label{softsoftmax} 
\end{alignat}

خروجی‌های نرم در مدل‌های آموزگار و دانش‌آموز و نیز خروجی‌های مطلوب\LTRfootnote{Ground Truth} نقش مهمی در آموزش دانش‌آموز دارند و با داشتن این مقادیر می‌توان تابع‌ هزینه‌ی مربوط به چگالش دانش\LTRfootnote{Distillation Loss} و 
تابع هزینه‌ی دانش‌آموز\LTRfootnote{Student Loss} را تعریف نمود. تابع هزینه‌ی چگالش دانش را می‌توان به صورت زیر میان لاجیت‌های آموز‌گار و دانش‌آموز به صورت یک تابع هزینه‌ی \lr{Cross-Entropy} نوشت. 

\begin{alignat}{5}
	L_D(p(z_t,T), p(z_s,T)) = -\Sigma_i p_i(z_{ti},T) \log(p_i(z_{si},T))    \label{ld} 
\end{alignat}

که در آن $z_t$ و $z_s$ به ترتیب، لاجیت‌های آموزگار و دانش‌آموز هستند. گرادیان تابع‌ هزینه‌ی چگالش نسبت به لاجیت‌های دانش‌آموز می‌تواند به صورت زیر محاسبه گردد.

\begin{alignat}{5}
	\frac{\partial L_D(p(z_t,T), p(z_s,T))}{\partial z_{si}} = \frac{p(z_t,T) - p(z_s,T)}{T}    
	= \frac{1}{T}(\frac{exp(z_{si}/T)}{\Sigma_j exp(z_{sj}/T)}  -\frac{exp(z_{ti}/T)}{\Sigma_j exp(z_{tj}/T)}) \label{gradld}
\end{alignat}

اگر فاکتور دما (T) بسیار بزرگتر از مقدار لاجیت‌ها باشد آنگاه با استفاده از بسط تیلور معادله‌ی ~\رجوع{gradld} را می‌توان به صورت زیر بازنویسی نمود.
\begin{alignat}{5}
	\frac{\partial L_D(p(z_t,T), p(z_s,T))}{\partial z_{si}} = \frac{1}{T}(\frac{1+\frac{z_{si}}{T}}{N+\Sigma_j \frac{z_{sj}}{T}} - \frac{1+\frac{z_{ti}}{T}}{N+\Sigma_j \frac{z_{tj}}{T}})    
	\label{gradsimplelddd}
\end{alignat}

با فرض میانگین صفر بودن لاجیت‌های آموزگار و دانش آموز معادله‌ی فوق را می‌توان ساده‌تر نمود.
\begin{alignat}{5}
	\frac{\partial L_D(p(z_t,T), p(z_s,T))}{\partial z_{si}} = \frac{1}{NT^2}(z_{si}-z_{ti})    
	 \label{gradsimpleld}
\end{alignat}

بنابراین با توجه به معادله‌ی ~\رجوع{gradsimpleld} هدف تابع هزینه‌ی چگالش، کمینه کردن فاصله‌ی میان لاجیت‌های آموزگار و شاگرد است \مرجع{hinton2015distilling}. تابع هزینه‌ی دانش‌آموز را نیز می‌توان به صورت \lr{Cross-Entropy} بین خروجی مطلوب و لاجیت‌های دانش‌آموز به صورت زیر نوشت.
\begin{alignat}{5}
	L_S(y, p(z_s,T)) = -\Sigma_i y \log(p_i(z_{si},T))    \label{ls} 
\end{alignat}

در نهایت، با کنار هم قرار دادن تابع‌ هزینه‌‌ی چگالش و تابع هزینه‌ی دانش‌آموز، تابع هزینه‌ی کلی برای چارچوب چگالش دانش از آموزگار به شاگرد طبق معادله‌ی ~\رجوع{losstotal} تشکیل می‌شود که با ضریب $\alpha$ مجموع این دو تابع به صورت وزن‌دار محاسبه می‌شود.

\begin{alignat}{5}
	L_{TotalDistillation} = \alpha L_D(p(z_t,T), p(z_s,T)) + (1-\alpha) L_S(y, p(z_s,T))   \label{losstotal} 
\end{alignat}

فاکتور دمای T در تابع هزینه‌ی دانش‌آموز برابر با یک و در تابع‌ هزینه‌ی چگالش برابر یا بزرگتر از یک در نظر گرفته می‌شود. بنابراین به صورت کلی، چارچوب چگالش دانش را می‌توان به صورت شکل ~\رجوع{شکل:نالجدیس} بیان نمود که در آن، آموزگار یک مدل از پیش آموزش دیده شده‌است و دانش‌آموز به واسطه‌ی خروجی‌های نرم و مقادیر مطلوب که دو تابع هزینه را تشکیل می‌دهند، آموزش داده می‌شود \مرجع{gou2021knowledge}.

\شروع{شکل}[H]
\centerimg{04knowledgedistillationframework.png}{15cm}
\شرح{عملکرد انتقال دانش توسط چارچوب چگالش دانش \مرجع{gou2021knowledge}}
\برچسب{شکل:نالجدیس}
\پایان{شکل}


\زیرزیرقسمت{معماری شبکه‌های قطعه‌بند}

همانطور که پیش‌تر اشاره گردید، در این قسمت، قصد داریم با استفاده از چارچوب چگالش دانش در بحث قطعه‌بندی ساختارهای در ریسک استفاده کنیم. برای این‌کار نیاز به مدل‌های آموزگار و دانش‌آموز است که در ابتدا یک مدل پیچیده با توانایی بالا بر روی دادگان به عنوان آموزگار آموزش می‌بیند و سپس مدل ساده‌تر (دانش‌آموز) با استفاده از چارچوب چگالش دانش، از دانش مدل آموزگار برای بهبود عملکرد بهره می‌برد.

برای مدل آموزگار از معماری یک UNet پیچیده با تعداد پارامتر‌های قابل آموزش بالا و روش‌های تعمیم‌پذیری استفاده گردید. معماری این شبکه‌ی تمام کانوولوشنی عمیق در شکل ~\رجوع{شکل:تیچریونت} قابل مشاهده است.

\شروع{شکل}[H]
\centerimg{04teacherUnet.png}{15cm}
\شرح{معماری شبکه‌ی UNet پیچیده، استفاده شده به عنوان آموزگار}
\برچسب{شکل:تیچریونت}
\پایان{شکل}

تفاوت این معماری با معماری UNet اولیه‌ی معرفی شده در \مرجع{ronneberger2015u} استفاده از ابزارهای افزایش قدرت تعمیم‌پذیری مانند dropout و Batch-normalization پس از لایه‌های کانوولوشنی است. همچنین برای افزایش وسعت دید شبکه و استخراج ویژگی‌ها عمیق‌تر  و کلی‌تر از سطح تصویر، اندازه‌ی فیلتر‌های کانوولوشنی برابر با $13*13$ تعریف گردید.

در ادامه، برای تعریف مدل دانش‌آموز، از دو معماری ساده‌تر استفاده شد. دانش‌آموز اول یک مدل UNet بسیار ساده‌تر با تعداد پارامترهای بسیار کمتر از UNet پیچیده‌ی آموزگار و دانش‌آموز دوم بک معماری بر اساس بلوک‌های Residual است. شکل ~\رجوع{شکل:اسوانیونت} معماری یک UNet ساده شده را به عنوان دانش‌آموز اول نشان می‌دهد که در مقایسه با معماری آموزگار در شکل ~\رجوع{شکل:تیچریونت}، ساختار ساده‌تری دارد که عبارتند از: کاهش ابعاد فیلترهای کانوولوشنی به $3*3$ و در نتیجه کاهش وسعت دید هر فیلتر، استفاده نکردن از روش‌های تعمیم‌پذیری dropout و Batch-normalization و در نهایت کاهش تعداد ویژگی‌های استخراج شده به نصف، نسبت به UNet پیچیده. 

\شروع{شکل}[H]
\centerimg{04simpleUnet.png}{10cm}
\شرح{معماری شبکه‌ی UNet ساده، استفاده شده به عنوان دانش‌آموز اول}
\برچسب{شکل:اسوانیونت}
\پایان{شکل}

شکل ~\رجوع{شکل:استو}، معماری دانش‌آموز دوم را که بر اساس بلوک‌های Residual طراحی شده است را نشان می‌دهد. که دوباره، نسبت به مدل آموزگار، دارای تعداد ویژگی نصف در هر لایه‌ است منتها برای آموزش بهتر آن از روش Batch-normalization نیز استفاده شده است.

\شروع{شکل}[H]
\centerimg{04Resnetstudent.png}{15cm}
\شرح{معماری شبکه‌ی پیشنهادی بر اساس بلوک‌های Residual، استفاده شده به عنوان دانش‌آموز دوم}
\برچسب{شکل:استو}
\پایان{شکل}

\زیرزیرقسمت{آموزش شبکه‌های قطعه‌بند}

برای آموزش شبکه‌های معرفی شده در بالا، از مجموعه دادگان ساختارهای در ریسک قفسه‌ی سینه‌ی SegTHOR معرفی شده در بخش ~\رجوع{segthor} استفاده گردید. همانظور که گفته شد، این مجموعه داده قطعه‌بندی چهار ساختار در ریسک را در 40 تصویر سی‌تی اسکن قفسه‌ی سینه تهیه نموده است. بنابراین برای قطعه‌بندی این چهار ساختار، همانطور که در شکل‌های ~\رجوع{شکل:تیچریونت} و ~\رجوع{شکل:اسوانیونت} نشان داده شده است، در لایه‌ی خروجی این معماری‌ها چهار خروجی گرفته شده است که هر یک از آن‌ها مسوول قطعه‌بندی یکی از ساختارهاست.

برای آموزش بر اساس چارچوب چگالش دانش لازم است تابع فعالیت خروجی برای کلاس‌ها Softmax باشد که با توجه به چهار ساختار در ریسک قلب، آئورت، نای و مری یک کلاس پس‌زمینه نیز به این چهار کلاس اضافه می‌شود تا بتوان از تابع هزینه‌ی \lr{Cross-Entropy} استفاده نمود. اما با آزمایش‌های اولیه‌ای که صورت گرفت، ملاحظه گردید در نظر نگرفتن پس‌زمینه به عنوان یک کلاس مجزا و فقط قطعه‌بندی چهار ساختار در ریسک، دقت بالاتری را بوجود می‌آورد بنابراین معماری شبکه‌ها برای ایجاد خروجی، تنها برای ساختارهای در ریسک طراحی گردید.

در این حالت، مشکلی که بوجود می‌آید عدم توانایی در استفاده از تابع هزینه‌ی \lr{Cross-Entropy} است زیرا دیگر طبقه‌ی اضافی پس زمینه وجود ندارد. برای حل این مشکل از تابع هزینه‌ی \lr{Binary Cross-Entropy} استفاده شد که به صورت زیر تعریف می‌شود:

\begin{alignat}{5}
	BCE = -\frac{1}{N}\sum_{i=1}^N y_i \log(p_i)+(1-y_i)  \log(1-p_i)  \label{bce} 
\end{alignat}

که در آن $y_i$ خروجی مطلوب که صفر یا یک است و $p_i$ خروجی پیش‌بینی شده است که در بازه‌ی صفر و یک است. فرض کنید اگر خروجی مطلوب برابر با صفر باشد، ترم اول در معادله‌ی ~\رجوع{bce} حذف می‌شود و ترم دوم سعی می‌کند با کمینه‌کردن مقدار هزینه، مقدار پیش‌بینی شده را به صفر نزدیک کند. بنابراین با صفر بودن دایمی یکی از کلاس‌ها با استفاده از این تابع هزینه، مشکلی در یادگیری بوجود نمی‌آید و با حل کردن این مساله‌ی بهینه‌سازی برای کلاس پس‌زمینه، مقدار $0.25$ در هر یک از نقشه‌های احتمال چهار ساختار در ریسک، بدست می‌آید.

برای آموزش شبکه‌ها به صورت تصادفی 80 درصد دادگان (32 تصویر سی‌تی اسکن) به عنوان دادگان آموزش و مابقی دادگان به عنوان دادگان آزمایش جدا شدند. بنابراین با توجه به موارد گفته شده، در ابتدا مدل آموزگار با استفاده از تابع هزینه‌ی \lr{Binary Cross-Entropy} و روش بهینه‌سازی Adam و با روش افزایش داده‌ی توضیح داده شده، آموزش داده شد. سپس با توجه به چارچوب چگالش دانش، دو مدل دانش‌آموز با همان دادگان آموزشی آموزگار، آموزش دیدند.

برای آموزش، دو مدل دانش‌آموز مطابق شکل ~\رجوع{شکل:مایدیستیل} دادگان آموزش ورودی به مدل از پیش آموزش دیده‌ی آموزگار و دانش‌آموز وارد می‌شود و با دریافت لاجیت‌های خروجی هر مدل توابع هزینه‌ی چگالش و دانش‌آموز تعریف خواهد شد.

\شروع{شکل}[H]
\centerimg{04distillationframework.png}{9cm}
\شرح{آموزش شبکه‌های دانش‌آموز برمبنای چارچوب چگالش دانش از آموزگار}
\برچسب{شکل:مایدیستیل}
\پایان{شکل}

به طور معمول برای تعریف تابع هزینه‌ی چگالش بین خروجی مدل آموزگار و دانش‌آموز از فاصله‌ی KL\LRfootnoterule{Kullback-Leibler  (KL) divergence loss} استفاده می‌شود \مرجع{gou2021knowledge} که با معادله‌ی زیر تعریف می‌شود و سعی در کم کردن فاصله‌ی بین دو توزیع احتمال را دارد. 

\begin{alignat}{5}
	L_D(p(z_s), p(z_t)) = L_{KL}(p(z_s), p(z_t)) = \sum p(z_s) \log(\frac{p(z_s)}{p(z_t)})    \label{kldivergence} 
\end{alignat}

بنابراین با استفاده از فاصله‌ی KL به عنوان تابع هزینه‌ی چگالش دانش و تابع هزینه‌ی BCE به عنوان تابع هزینه‌ی میان پیش‌بینی دانش‌آموز و مقادیر مطلوب، هزینه‌ی کلی به صورت زیر تعریف گردید.
 
\begin{alignat}{5}
	L_{total} = \alpha BCE(p(z_s), y) + (1-\alpha)L_{KL}(p(z_s,T), p(z_t,T))     \label{bceklloss} 
\end{alignat}

با ملاحظات گفته‌شده در بالا و تنظیم پارامترهای T برابر با 2 و $\alpha$ برابر با $0.2$ آموزش دو شبکه‌ی دانش‌آموز با چارچوب چگالش دانش از آموزگار صورت گرفت. برای مقایسه‌ی عملکرد این چارچوب، مدل‌های دانش‌آموز یک‌ بار دیگر با همان دادگان آموزش برای آموزگار و شرایط یکسان منتها به صورت معمول و تنها تابع هزینه‌ی BCE آموزش داده شدند. در ادامه به بررسی نتایج حاصل پرداخته خواهد شد.

\زیرزیرقسمت{نتایج}

برای محاسبه‌ی معیارهای ارزیابی باید برچسب‌های احتمالاتی پس از تابع فعالیت Softmax آستانه‌گذاری شوند و برخلاف تابع فعالیت Sigmoid که آستانه‌ی مشخص $0.5$ را دارد، در این حالت باید آستانه‌های بهینه برای بدست آوردن بالاترین مقدار در معیارهای قطعه‌بندی استخراج گردد. برای این‌کار، پیش‌بنی‌های احتمالاتی برای مدل آموزگار، بر روی مجموعه‌ داده‌ی آموزش استخراج گردید و با آستانه‌گذاری‌های مختلف، معیار Dice برای دادگان آموزش و چهر ارگان در ریسک استخراج گردید. نمودار زیر تاثیر مقدار آستانه را بر روی مقدار معیار Dice برای هر ساختار نشان می‌دهد.

\شروع{شکل}[H]
\centerimg{04thresholdsVSDice.png}{9cm}
\شرح{تاثیر تغییر آستانه بر روی معیار Dice برای هر ساختار در ریسک}
\برچسب{شکل:ترشولددایس}
\پایان{شکل}

همانطور که در شکل ~\رجوع{شکل:ترشولددایس} ملاحظه‌ می‌شود مقدار آستانه بهینه برای هر ساختار متفاوت است. برای استخراج این آستانه‌ی بهینه از روش آستانه‌گذاری اوتسو، که در فصل قبل توضیح داده شد، استفاده گردید، به این ترتیب آستانه‌ی بهینه برای مری، $0.71$، قلب، $0.93$، نای، $0.86$ و آئورت، $0.82$ بدست آمد. بنابراین با این آستانه‌گذاری ماسک‌های صفر و یک، برای چهار ارگان در ریسک ذکر شده بر روی دادگان آزمایش برای تمامی مدل‌ها بدست آمد. جدول‌های زیر سه معیار ارزیابی Dice، Jaccard و Hausdorff را برای چهار ساختار در ریسک، نشان می‌دهد. در سه ردیف نخست، ارزیابی عددی برای شبکه‌های آموزگار، دانش‌آموز Resnet (شبکه‌ی مبتنی بر بلوک‌های Residual) و دانش‌آموز UNet ساده قرار دارند. در دو ردیف انتهایی، دو مدل دانش‌آموز به صورت تنها و بدون استفاده از چهارچوب چگالش دانش‌، ارزیابی شده‌اند.

\begin{table}[H]
	\caption{معیار Dice برای مدل‌های مختلف}
	\label{distilldice}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\multicolumn{4}{l}{نام ساختار در ریسک} \\
		
		\cmidrule(r){2-5}
		نام مدل & مری & قلب & نای & آئورت \\
		\midrule
		 UNet آموزگار & $0.73 \pm 0.13$ & $0.95 \pm 0.02$ & $0.93 \pm 0.03$ & $0.86 \pm 0.08$\\
		Resnet دانش‌آموز & $0.45 \pm 0.11$ & $0.62 \pm 0.03$ & $0.81 \pm 0.06$ & $0.63 \pm 0.17$ \\
		Unet دانش‌آموز & $0.62 \pm 0.13 $ & $0.91 \pm 0.04$ & $0.89 \pm 0.07 $ & $0.79 \pm 0.18$ \\
		\midrule
		Resnet تنها & $0.33 \pm 0.07 $ & $0.13 \pm 0.04 $ & $0.80 \pm 0.05$ & $0.22 \pm 0.05$ \\
		 Unet تنها & $0.54 \pm 0.13$ & $0.79 \pm 0.09$ & $0.88 \pm 0.05$ & $0.64 \pm 0.17$ \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[H]
	\caption{معیار Jaccard برای مدل‌های مختلف}
	\label{distilljaccard}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\multicolumn{4}{l}{نام ساختار در ریسک} \\
		
		\cmidrule(r){2-5}
		نام مدل & مری & قلب & نای & آئورت \\
		\midrule
		Unet آموزگار & $0.64 \pm 0.12$ & $0.93 \pm 0.03$ & $0.90 \pm 0.03$ & $0.82 \pm 0.09$\\
		Resnet دانش‌آموز & $0.38 \pm 0.09$ & $0.60 \pm 0.03$ & $0.77 \pm 0.07$ & $0.56 \pm 0.16$ \\
		Unet دانش‌آموز & $0.53 \pm 0.11 $ & $0.89 \pm 0.04$ & $0.86 \pm 0.07 $ & $0.74 \pm 0.19$ \\
		\midrule
		Resnet تنها & $0.29 \pm 0.05 $ & $0.09 \pm 0.03 $ & $0.76 \pm 0.05$ & $0.22 \pm 0.04$ \\
		Unet تنها & $0.45 \pm 0.11$ & $0.77 \pm 0.09$ & $0.85 \pm 0.06$ & $0.56 \pm 0.15$ \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]
	\caption{فاصله‌ی Hausdorff برای مدل‌های مختلف}
	\label{distillhausdorff}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\multicolumn{4}{r}{نام ساختار در ریسک} \\
		
		\cmidrule(r){2-5}
		نام مدل & مری & قلب & نای & آئورت \\
		\midrule
		Unet آموزگار & $4.54 \pm 0.61$ & $8.11 \pm 1.48$ & $4.16 \pm 0.44$ & $6.12 \pm 0.61$\\
		Resnet دانش‌آموز & $4.74 \pm 0.53$ & $10.34 \pm 0.56$ & $4.52 \pm 0.54$ & $7.64 \pm 1.16$ \\
		Unet دانش‌آموز & $4.60 \pm 0.54 $ & $8.52 \pm 1.49$ & $4.45 \pm 0.28 $ & $6.02 \pm 0.77$ \\
		\midrule
		Resnet تنها & $4.77 \pm 0.48 $ & $12.80 \pm 1.15 $ & $4.32 \pm 0.36$ & $9.00 \pm 0.83$ \\
		Unet تنها & $4.70 \pm 0.56$ & $9.50 \pm 1.49$ & $4.52 \pm 0.18$ & $6.63 \pm 0.86$ \\
		\bottomrule
	\end{tabular}
\end{table}

یک نمونه از خروجی قطعه‌بندی تصاویر سی‌تی اسکن سه‌بعدی دادگان آزمایش، در چهار نمای مختلف اکسیاب، سجیتال، کرونال و سه‌بعدی برای مدل‌های مختلف در شکل ~\رجوع{شکل:دیستیلپرید} نشان داده شده است که در آن (الف) قطعه‌بندی مطلوب، (ب) پیش‌بینی مدل آموزگار، (پ) پیش‌بینی دانش‌آموز Resnet (ت) پیش‌بینی دانش‌آموز UNet، (ث) پیش‌بینی Resnet تنها و (ج) پیش‌بینی UNet ساده‌ی تنها هستند.

\شروع{شکل}[H]
\centerimg{04distillationoutputs.png}{15cm}
\شرح{خروجی مدل‌های مختلف به صورت آموزش داده‌ شده با چارچوب چگالش دانش و تکی، (الف) قطعه‌بندی مطلوب، (ب) پیش‌بینی مدل آموزگار، (پ) پیش‌بینی دانش‌آموز Resnet (ت) پیش‌بینی دانش‌آموز UNet، (ث) پیش‌بینی Resnet تنها و (ج) پیش‌بینی UNet ساده‌ی تنها}
\برچسب{شکل:دیستیلپرید}
\پایان{شکل}

\زیرزیرقسمت{بحث و نتیجه‌گیری}

همانطور که اشاره گردید، هدف از چارچوب چگالش دانش، انتقال دانش یادگیری شده از آموزگار به دانش‌آموز است که این‌ روند باعث بهبود دقت و یادگیری بهتر در دانش‌آموز می‌شود. همانطور که در جدول‌های ~\رجوع{distilldice}، ~\رجوع{distilljaccard} و ~\رجوع{distillhausdorff} و شکل ~\رجوع{شکل:دیستیلپرید} ملاحظه‌ گردید، استفاده از این چارچوب باعث افزایش دقت عملکرد در قطعه‌بندی ساختارهای در ریسک بدون تغییر مدل و اضافه کردن هزینه‌ی محاسباتی می‌شود. برای مقایسه‌ی بهتر بهبود عملکرد نمودار جعبه‌ای\LTRfootnote{Box Plot} معیار Dice دادگان آزمایش برای چهار ساختار در ریسک و برای پنج شبکه‌ی آموزش داده شده به صورت یک‌جا در شکل ~\رجوع{شکل:باکسپلاتدایسدیستیل} رسم گردید. همانطور که ملاحظه می‌شود شبکه‌هایی دانش‌آموز که یکبار به صورت تکی و بار دیگر با چارچوب چگالش دانش آموزش دیده‌اند، عملکردشان بهبود یافته است.

\شروع{شکل}[H]
\centerimg{04boxplotdistill.png}{15cm}
\شرح{نمودار جعبه‌ای معیار Dice دادگان آزمایش برای چهار ساختار در ریسک و پنج شبکه‌ی آموزش داده شده}
\برچسب{شکل:باکسپلاتدایسدیستیل}
\پایان{شکل}

برای شهود بیشتر عملکرد چارچوب چگالش دانش، شکل ~\رجوع{شکل:هیتمپدیستیل} را در نظر بگیرید. در این شکل، نقشه‌ی احتمالاتی خروجی شبکه‌ها قبل از آستانه‌گذاری برای قلب نشان داده شده است. ملاحظه می‌شود که طبق انتظار، پس‌زمینه‌ی تصویر با احتمال $0.25$ در این نقشه ظاهر شده است و برای ساختارهای دیگر مانند نای، مری و آئورت، احتمال بسیار نزدیک به صفر است. نکته‌ی قابل توجه در نقشه‌ی احتمالاتی دانش‌آموزان با و بدون استفاده از چگالش دانش است که در در Resnet احتمال بالایی برای حضور قلب در سمت راست بدن ایجاد شده است اما بعد از چگالش دانش این مشکل رفع شده است.

\شروع{شکل}[H]
\centerimg{04heatmapdistill.png}{15cm}
\شرح{نقشه‌های احتمالاتی خروجی شبکه‌ها قبل از آستانه‌گذاری (الف)  آموزگار (ب) دانش‌آموز Resnet (پ) دانش‌آموز UNet (ت) Resnet تنها (ث) UNet تنها}
\برچسب{شکل:هیتمپدیستیل}
\پایان{شکل}

هدف چگالش دانش افزایش دقت در مدل‌های ضعیف‌تر است. برای مقایسه‌ی میزان پیچیدگی مدل‌ها از سه معیار تعداد پارامترهای قابل یادگیری (بر حسب میلیون)، تعداد عملیات اعشاری در واحد زمان\LTRfootnote{Floating Point Operations Per Second (FLOPS)} (بر حسب میلیارد) و متوسط زمان اجرا بر روی تصاویر سه‌بعدی (بر حسب ثانیه) در جدول ~\رجوع{modelscomplexity} نشان داده شده است که حاکی از پیچیدگی بسیار زیاد مدل آموزگار نسبت به مدل‌های دانش‌آموز است. اما با استفاده از روش چگالش دانش، ملاحظه گردید که بدون افزایش پیچیدگی مدل می‌توان دقت آن را به دقت بالاتری رسانید و از هزینه‌های محاسباتی بالا و زمان پردازش زیاد جلوگیری نمود.


\begin{table}[H]
	\caption{مقابسه‌ی میزان پیچیدگی و هزینه‌ی محاسباتی هر مدل}
	\label{modelscomplexity}
	\centering
	\begin{tabular}{lccc}
		\toprule
		نام شبکه & تعداد پارامترها (میلیون) & FLOPs(G) &زمان اجرا (ثانیه) \\
		\midrule
		Unet آموزگار & $33.542$ & $183$ & $14.2$ \\
		Resnet & $0.083$ & $18.1$ & $3.54$ \\
		Unet ساده & $0.540$ & $2.71$ & $2.26$ \\
		
		\bottomrule
	\end{tabular}
\end{table}

%####################################################################################
%####################################################################################
%####################################################################################
\زیرقسمت{بازخورد خطای پیش‌بین برای قطعه‌بندی ساختار‌های در ریسک}

همانطور که می‌دانید، عملکرد شبکه‌های عمیق علی‌الخصوص شبکه‌های کانوولوشنی عمیق بر اساس عملکرد کورتکس بینایی انسان طراحی شده است. از بین مدل‌هایی که برای عملکرد سیستم عصبی ارایه شده است، مدل کدگذاری پیش‌بین\LTRfootnote{Predictive Coding Model}  بر اساس شبکه‌های بازگشتی\LTRfootnote{Recurrent Networks}، یک مسیر بازخورد از لایه‌های بالاتر به لایه‌ها پایین‌تر که حاوی پیش‌بینی عملکرد ناحیه‌های پایینی کورتکس است، متصل می‌شود و در مسیر روبه‌جلو، خطای پیش‌بینی شده جریان می‌یابد \مرجع{rao1999predictive}.  

با الگوبرداری از این مدل، در این قسمت قصد داریم با اضافه کردن یک مسیر پیش‌بین به شبکه‌های روبه‌جلوی کانوولوشنی و بازخورد دادن خطای پیش‌بینی، یک چارچوب برای افزایش دقت شبکه‌های قطعه‌بند برای قطعه‌بندی ساختارهای در ریسک ارایه دهیم.

پیش‌تر اشاره گردید که مفهوم و عملکرد شبکه‌های کانوولوشنی عمیق از عملکرد مغز الگو برداری شده است اما بر خلاف مدل‌ کدگذاری پیش‌بین، فقط یک مسیر رو به جلو در طراحی این شبکه‌ها لحاظ شده است در صورتی که در عملکرد کورتکس بینایی مسیرهای بازگشتی برای بازخورد خطا از لایه‌های بالاتر به لایه‌های پایین‌تر نیز وجود دارد. برای رفع این مشکل، چارچوب بازخورد خطای پیش‌بین در این مطالعه ارایه گردیده است.

\زیرزیرقسمت{معماری چارچوب و آموزش مدل}

چارچوب بازخورد خطای پیش‌بین، از یک شبکه‌ی رمزگذار (Encoder) و دو شبکه‌ی رمزگشا (Decoder) تشکیل شده‌است. برای شهود بیشتر شکل ~\رجوع{شکل:پیپفریمورک} را در نظر بگیرید که در آن قصد، قطعه‌بندی ساختارهای در ریسک لایه‌های دوبعدی تصاویر سه‌بعدی سی‌تی اسکن است. این چارچوب از سه مسیر که در شکل نشان داده شده‌اند، تشکل شده است:

\شروع{فقرات}
\فقره مسیر اول یا مسیر بازسازی ورودی
\فقره مسیر بازخورد خطای پیش‌بین
\فقره مسیر دوم یا مسیر پیش‌بینی قطعه‌بندی
\پایان{فقرات}

\شروع{شکل}[H]
\centerimg{04EFbNet.png}{15cm}
\شرح{عملکرد چارچوب بازخورد خطای پیش‌بین برای قطعه‌بندی ساختارهای در ریسک}
\برچسب{شکل:پیپفریمورک}
\پایان{شکل}

در مسیر اول ابتدا تصویر ورودی به شبکه‌ی کدگذار وارد می‌شود و بردارهای ویژگی استخراج شده از آن، به شبکه رمزگشای اول (قسمت بازسازی) وارد می‌شود تا از تصویر ورودی یک بازسازی حاصل شود. سپس این بازسازی با تصویر اولیه مقایسه‌ می‌شود تا نواحی‌ای که به اشتباه بازسازی شده‌اند مشخص شوند. این اختلاف بازسازی با یک ضریب با تصویر پیش‌بینی شده، جمع می‌شود و از طریق مسیر بازخورد به قسمت شبکه‌ی رمزگذار بازخورد داده می‌شود. تا این قسمت از روند را می‌توان به صورت زیر فرمول‌بندی نمود:

\begin{alignat}{5}
	&X' = Decoder_1(Encoder(X))    \label{recpep} && \\
	&Z = X' + \beta|X'-X| && \notag
\end{alignat}

که در آن $X$ تصویر ورودی، $X'$ تصویر بازسازی شده، $Z$ تصویر شامل‌ خطای پیش‌بینی در مسیر بازخورد و $\beta$ ضریب ثابت برای جمع وزن‌دار تصویر پیش‌بینی با خطای بازسازی است.

در گام بعدی از مسیر دوم، مجموع وزن‌دار خطای پیش‌بینی و تصویر پیش‌بینی شده وارد شبکه‌ی کدگذار و سپس از قسمت کدگشای شبکه‌ی قطعه‌بند عبور می‌کند تا قطعه‌بندی نهایی تولید گردد. عملکرد این قسمت نیز به شکل زیر فرمول‌بندی می‌شود.

\begin{alignat}{5}
	&P = Decoder_2(Encoder(Z))    \label{segpep} &&
\end{alignat}

که در آن $P$ خروجی قطعه‌بندی نهایی است. برای آموزش یک شبکه توسط این چارچوب، لازم است دو تابع هزینه تعریف گردد، تابع هزینه‌ی شبکه بازسازی کننده که باید فاصله‌ی میان تصویر بازسازی شده و تصویر اولیه را کمینه کند و تابع هزینه‌ی شبکه قطعه‌بندی که هزینه‌ی بین ماسک قطعه‌بندی خروجی و قطعه‌بندی مطلوب را کمینه می‌کند. برای تابع هزینه‌ی باز سازی می‌توان از میانگین مربعات خطا\LTRfootnote{Mean Square    Error} (MSE) و برای تابع هزینه‌ی قطعه‌بندی از BCE استفاده کرد. جمع وزن‌دار این دو تابع هزینه، هزینه‌ی نهایی را ایجاد می‌کند. معادلات  ~\رجوع{pepfinalloss} تابع هزینه‌ی این چارچوب را به صورت فرمول‌بندی نشان می‌دهد.

\begin{alignat}{5}
	&MSE(X, X') = \frac{1}{N} \sum_{i=1}^{N}(X_i-X_i')^2    \notag  && \\
	&Loss = \alpha MSE(X, X') + BCE(Y, P) && \label{pepfinalloss}
\end{alignat}

برای ارزیابی این چارچوب از معماری یک UNet مطابق با شکل ~\رجوع{شکل:پیپیونت} استفاده گردید. برای شبکه‌های رمزگذار و رمزگشای معرفی شده در شکل ~\رجوع{شکل:پیپفریمورک}، از قسمت‌های Encoder و  Decoder معماری شبکه‌ی زیر استفاده گردید. که Encoder برای دو شبکه‌ی Decoder به صورت مشترک عمل می‌کند. مسیرهای اتصالی از Encoder به Decoder‌ها همچنان در چارچوب معرفی شده برای استفاده از شبکه‌ی UNet همچنان پا برجاست.

\شروع{شکل}[H]
\centerimg{04pepunet.png}{12cm}
\شرح{معماری شبکه‌ی UNet استفاده شده برای آموزش با چارچوب بازخورد خطای پیشبین}
\برچسب{شکل:پیپیونت}
\پایان{شکل}

برای آموزش این شبکه، از مجموعه دادگان SegTHOR با شرایط گفته شده در بخش‌های قبل استفاده گردید. پارامترهای $\alpha$ و $\beta$ در معادلات ~\رجوع{pepfinalloss} و ~\رجوع{recpep} به ترتیب برابر با $0.2$ و $2$ در روند آموزش قرار داده شد.

\زیرزیرقسمت{نتایج و ملاحظات}

شبکه‌ی UNet یک‌ مرتبه به صورت تکی و یک بار با استفاده از چارچوب بازخورد خطای پیش‌بین، آموزش داده شد. معیارهای ارزیابی برای این دو شبکه به صورت زیر بر روی دادگان آزمایش استخراج گردید.


\begin{table}[H]
	\caption{معیارهای ارزیابی قطعه‌بندی برای شبکه‌ی آموزش دیده شده به دو صورت تکی و چارچوب بازخورد خطای پیشبین}
	\label{peptableresults}
	\begin{tabular}{lllllll}
		\hline
		\multirow{2}{*}{نام شبکه}      & \multirow{2}{*}{معیار} & \multicolumn{4}{r}{نام ساختار در ریسک}                & \multirow{2}{*}{میانگین} \\ \cline{3-6}
		&                        & مری   & قلب       & نای     & آئورت       &                          \\ \hline
		\multirow{3}{*}{Unet با چارچوب}          & Dice                    & $0.68 \pm 0.08$  & $0.88\pm0.04$   & $0.88\pm0.03$   & $0.82\pm0.13$   & $0.82\pm0.07$                \\ \cline{2-7} 
		& Hausdorff                   & $4.63\pm0.73$   & $8.37\pm1.06$   & $4.67\pm0.40$   & $6.16\pm1.01$   & $5.96\pm0.80$                \\ \cline{2-7} 
		& Jaccard                  & $0.58\pm0.08$   & $0.87\pm0.04$   & $0.85\pm0.04$   & $0.76\pm0.14$   & $0.77\pm0.07$                \\ \hline
		\multirow{3}{*}{Unet تنها} & Dice                   & $0.54 \pm 0.13$ & $0.79 \pm 0.09$ & $0.88 \pm 0.05$ & $0.64 \pm 0.17$ & $0.71\pm0.11$                \\ \cline{2-7} 
		& Hausdorff                  & $4.70 \pm 0.56$ & $9.50 \pm 1.49$ & $4.52 \pm 0.18$ & $6.63 \pm 0.86$ & $6.34\pm0.77$                \\ \cline{2-7} 
		& Jaccard                  & $0.45 \pm 0.11$ & $0.77 \pm 0.09$ & $0.85 \pm 0.06$ & $0.56 \pm 0.15$ & $0.66\pm0.10$                \\ \hline
	\end{tabular}
\end{table}

شکل ~\رجوع{شکل:پیپاوتس} پیش‌بینی دو شبکه‌ی UNet آموزش دیده شده به صورت تکی (ب) و با چارچوب بازخورد خطای پیش‌بین (پ) را در کنار قطعه‌بندی مطلوب (الف) در نماهای مختلف نشان می‌دهد. 

\شروع{شکل}[H]
\centerimg{04pepoutputs.png}{12cm}
\شرح{پیش‌بینی قطعه‌بندی ساختارهای در ریسک در نماهای مختلف (الف) خروجی مطلوب (ب) خروجی UNet تنها (پ) خروجی UNet آموزش دیده شده با چارچوب بازخورد خطای پیش‌بین}
\برچسب{شکل:پیپاوتس}
\پایان{شکل}

همانطور که در شکل ~\رجوع{شکل:پیپاوتس} و جدول ~\رجوع{peptableresults} ملاحظه‌ می‌شود با استفاده کردن از چارچوب بازخورد خطای پیشبین، دقت قطعه‌بندی برای ساختارهای در ریسک افزایش خواهد یافت (میانگین معیار Dice برای چهار ارگان در ریسک $0.11$ رشد داشته است). علت این بهبود در استفاده از این چارچوب را می‌توان در دو دلیل توضیح داد:

\شروع{فقرات}
\فقره استفاده از یک Encoder مشترک در پیش‌بینی ورودی و قطعه‌بندی آن با توابع هزینه‌های مختلف باعث هدایت کردن Encoder به سمت استخراج ویژگی‌های سطح بالا و در نتیجه بهبود عملکرد قسمت‌ قطعه‌بندی می‌شود.

\فقره با جمع کردن خطای پیش‌بینی شده با تصویر بازسازی شده و عبور دادن این تصویر برای قطعه‌بندی، ناحیه‌هایی که شبکه توانایی بازسازی آن‌ها نداشته است مورد تاکید قرار می‌گیرند و در مسیر دوم باعث بهبود دقت قطعه‌بندی خواهند شد.
\پایان{فقرات}

بنابراین در این قسمت یک چارچوب برای آموزش شبکه‌های عصبی عمیق با رویکرد بازخورد خطا برای افزایش دقت قطعه‌بندی ارایه گردید و ملاحظه شد، با آموزش یک UNet با استفاده از این چارچوب دقت به صورت قابل ملاحظه‌ای افزایش یافت.
%####################################################################################
%####################################################################################
%####################################################################################
\زیرقسمت{معرفی تابع هزینه‌ی بر اساس شکل برای قطعه‌بندی ساختارهای در ریسک}

ابزارهای یادگیری عمیق نسبت به ابزارهای کلاسیک دیگر در حوزه‌ی قطعه‌بندی تصاویر پزشکی، دقت بسیار بالاتری نشان داده است. عناصر سازنده‌ی یک الگوریتم یادگیری عمیق از موارد زیر تشکیل شده است \مرجع{fidon2020generalized}.

\شروع{فقرات}
\فقره معماری شبکه با تعداد پارامتر قابل یادگیری مناسب برای تشکیل نگاشت غیرخطی بین ورودی و خروجی
\فقره تایع هزینه‌ی مناسب برای آموزش شبکه
\فقره دادگان آموزش مناسب 
\فقره الگوریتم بهینه‌سازی برای تنظیم وزن‌های شبکه
\پایان{فقرات}

هر گونه ضعف در یکی از موارد بالا باعث ایجاد خروجی نامطلوب در پیش‌بینی شبکه‌ی عصبی عمیق می‌شود. یکی از موارد ضعف این مدل‌ها که در شکل‌های ~\رجوع{شکل:دیستیلپرید} و ~\رجوع{شکل:پیپاوتس} مشاهده می‌شود وجود جزیره‌ای‌ از قطعه‌بندی‌های نامطلوب یا سوراخ‌هایی در یک بافت یکپارچه است. مطالعات اخیر نشان می‌دهد، برخلاف توانمندی بسیار بالای مدل‌های یادگیری عمیق در استخراج ویژگی‌های سطح بالا، این مدل‌ها توانایی یادگیری شکل و ساختار بافت‌ها را ندارند \مرجع{klyuzhin2022testing}. علت این کاستی را در تابع هزینه‌ی طراحی شده برای شبکه‌های عصبی می‌توان یافت. این توابع هزینه معمولا به صورت پیکسل به پیکسل عمل می‌کنند (مانند BCE) و ویژگی‌های مربوط به شکل و ساختار عضو مورد مطالعه را در نظر نمی‌گیرند.

بنابراین این مشکل در خروجی قطعه‌بندی باعث پدید آمدن کاستی‌هایی مانند، تکه‌تکه شدن ساختارهای یکپارچه، پدید آمدن جزیره‌های واکسلی و ناهمگونی شکل در ساختار قطعه‌بندی شده، می‌شود. روش‌های بسیاری برای حل این مشکل از طریق ایجاد یک دانش اولیه\LTRfootnote{Prior Knowledge} برای قطعه‌بندی و یا روش‌های پس‌پردازش\LTRfootnote{Post-Processing} ارایه شده است که از جمله‌ی آن‌ها می‌توان به فیلد‌های تصادفی شرطی/مارکوف\LTRfootnote{Conditional/Markov Random Fields} و مدل‌های کانتور فعال\LTRfootnote{Active Contour Models} اشاره کرد \مرجع{bohlender2021survey}.

همانطور که گفته شد این روش‌ها با تاثیر بر روی ورودی یا خروجی نهایی شبکه‌ی عصبی سعی در حل مشکلات ذکر شده دارند. اما هنوز مشکل حل نشده باقی مانده است و شبکه ویژگی‌های مربوط به شکل و ساختار را یاد نگرفته است. تا کنون تلاش‌هایی برای اجبار شبکه به یادگیری شکل شده است. محققی و همکاران \مرجع{mohagheghi2020incorporating} در مطالعه‌ای برای قطعه‌بندی سه‌بعدی کبد با شبکه‌های عصبی عمیق، یک چارچوب برای استخراج ویژگی‌های شکل معرفی کرده‌اند. رویکرد آن‌ها استفاده از یک شبکه‌ی دیگر ماسک قطعه‌بندی را بازسازی می‌کند و ویزگی‌های مربوط به این ماسک قطعه‌بندی شده‌ی سه‌بعدی را استخراج می‌کند سپس در شبکه‌ی قطعه‌بند یک تابع هزینه برای کمینه‌کردن فاصله‌ی ویژگی‌های استخراج شده از قطعه‌بندی مطلوب و قطعه‌بندی پیش‌بینی شده، اضافه نموده است. با وجود بهبود دقت در قطعه‌بندی، در این چارچوب، هنوز اطمینانی وجود ندارد که شبکه‌ی استخراج کننده‌ی ویژگی، ویژگی‌های مربوط به شکل را استخراج کرده است. بنابراین در این مطالعه سعی داریم تایع هزینه‌ای پیشنهاد کنیم که با اطمینان از ویژگی‌ها شکل و ساختار مورد نظر را استخراج و مقایسه کند.

ساختارهای در ریسک موجود در بدن، معمولا شکل و ساختار خاصی دارند و در بین بیماران مختلف تغییرات نسبتا کمی دارند. بنابراین با اجبار شبکه به یادگیری شکل این ساختارها می‌توان قطعه‌بندی با دقت بالاتری برای این ساختارها تولید نمود. 





%####################################################################################
%####################################################################################
%####################################################################################
\قسمت{روش‌های پیشنهادی قطعه‌بندی تومور}

\زیرقسمت{معرفی دادگان و پیش‌ پردازش}